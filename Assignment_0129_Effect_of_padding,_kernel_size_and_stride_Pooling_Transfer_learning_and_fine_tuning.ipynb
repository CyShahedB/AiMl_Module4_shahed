{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJWwNmSBYtRnAxPC1oy+l8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CyShahedB/AiMl_Module4_shahed/blob/main/Assignment_0129_Effect_of_padding%2C_kernel_size_and_stride_Pooling_Transfer_learning_and_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**EXERCISES**"
      ],
      "metadata": {
        "id": "tnfb5iNU5Dc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.Effect of padding, kernel size and stride"
      ],
      "metadata": {
        "id": "6FK5NULf5UOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EFFECT OF PADDING\n",
        "**Q:Change the padding value with the slider. What do you observe?**\n",
        "\n",
        "Ans:\n",
        "\n",
        "Observations:\n",
        "\n",
        "**Increasing Padding:** As you increase the padding using the slider, the output image will become larger. More padding adds extra pixels around the input image, preventing the convolution operation from reducing the size of the image. This leads to an increase in the spatial dimensions of the output.\n",
        "\n",
        "**Decreasing Padding:** Conversely, decreasing the padding will make the output image smaller. Less padding means that the convolution operation will not extend as far into the image, resulting in a smaller output.\n",
        "\n",
        "**Effect on Image Edges:** The padding value also influences how the convolution operation treats the edges of the image. More padding ensures that the convolutional filter can fully traverse the entire image, including the edges, while less padding may cause the edges to be less affected.\n",
        "\n",
        "**Visualization:** The code provides a visual representation of the result of the convolution operation, and you'll see the impact of different padding values on the displayed image. The shape of the output image will also change based on the chosen padding.\n",
        "\n",
        "Adjusting the padding value using the slider affects the spatial dimensions of the output image after the convolution operation and influences how the convolutional filter interacts with the edges of the input image.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dZnudZz17AbV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVThLy3OfBi4"
      },
      "source": [
        "# Effect of Kernel size\n",
        "**Q: Change the kernel size with the slider. What do you observe?**\n",
        "\n",
        "Ans:\n",
        "\n",
        "Observations:\n",
        "\n",
        "**Increasing Kernel Size:** As we increase the kernel size using the slider, we'll observe a larger receptive field. Larger kernels capture more complex patterns or structures in the image. However, using very large kernels may lead to increased computational cost and may also result in loss of fine details.\n",
        "\n",
        "**Decreasing Kernel Size:** Conversely, decreasing the kernel size will result in a smaller receptive field. Smaller kernels focus on capturing finer details and can be computationally more efficient. However, they might miss out on capturing larger and more complex patterns in the image.\n",
        "\n",
        "**Impact on Image Features:** The choice of kernel size influences the features that the convolutional operation can detect. Smaller kernels are sensitive to local features, while larger kernels can capture more global features.\n",
        "\n",
        "**Visualization:** The code provides a visual representation of the result of the convolution operation, and we'll see the impact of different kernel sizes on the displayed image. The shape of the output image will also change based on the chosen kernel size.\n",
        "\n",
        "Adjusting the kernel size using the slider affects the receptive field of the convolutional operation and, consequently, the types of features that can be captured from the input image."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Effect of Stride\n",
        "**Q: Change the stride value with the slider. What do you observe?**\n",
        "\n",
        "Ans:\n",
        "\n",
        "Observations:\n",
        "\n",
        "**Smaller Stride:**\n",
        "\n",
        "Pros: Smaller strides result in a more detailed representation of the input data and allow the model to capture fine-grained features. Cons: Smaller strides can lead to larger output feature maps, requiring more computation and memory. It may also increase the risk of overfitting.\n",
        "\n",
        "**Larger Stride:**\n",
        "\n",
        "Pros: Larger strides reduce the spatial dimensions of the output feature maps, leading to a more compact representation of the input. This can reduce computation and memory requirements.\n",
        "\n",
        "Cons: Larger strides may cause the model to miss fine-grained details in the input data.\n",
        "\n",
        "**Impact on Spatial Dimensions:**\n",
        "\n",
        "Smaller strides result in smaller reductions in spatial dimensions after convolution, while larger strides lead to more substantial reductions."
      ],
      "metadata": {
        "id": "rtX0eXN3EGWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "**1. Does increasing stride increase output image size?**\n",
        "\n",
        "Ans:\n",
        "\n",
        "No, increasing the stride does not increase the output image size; it actually reduces it.\n",
        "\n",
        "As you increase the stride value, the convolutional filter skips more pixels during each step, resulting in a smaller spatial dimensions of the output image. The output size decreases as the filter moves across the input with larger steps.\n",
        "\n",
        "The stride is a hyperparameter that determines the step size of the convolutional filter as it moves across the input image. A larger stride means the filter skips more pixels with each step, resulting in a downsampled output.\n",
        "\n",
        "**2. Does increasing padding increase output image size?**\n",
        "\n",
        "Ans:\n",
        "\n",
        "Yes, increasing padding does increase the output image size after a convolution operation.\n",
        "\n",
        "Increasing Padding: Adding more padding around the input image allows the convolutional filter to extend further into the image during the convolution operation. As a result, the spatial dimensions of the output feature map increase. More padding retains more spatial information and prevents the convolution from reducing the size of the image.\n",
        "\n",
        "Increasing the padding adds more pixels around the input, leading to a larger output image size."
      ],
      "metadata": {
        "id": "OyIzAnJz5lor"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W1bNXcJEK6d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. POOLING"
      ],
      "metadata": {
        "id": "ROW-fgeAI0df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "**1. Can you think of any other pooling other than max and avg?**\n",
        "\n",
        "Ans:\n",
        "\n",
        "**Min Pooling:**\n",
        "\n",
        "Similar to Max Pooling, but it takes the minimum value within the pooling window.\n",
        "\n",
        "**Global Average Pooling (GAP):**\n",
        "\n",
        "Instead of applying pooling operations on local regions, Global Average Pooling computes the average of each feature map across the entire spatial dimensions. It reduces each feature map to a single value.\n",
        "\n",
        "**Global Max Pooling:**\n",
        "\n",
        "Similar to Global Average Pooling, but it selects the maximum value across each feature map.\n",
        "\n",
        "**Fractional Pooling:**\n",
        "\n",
        "Fractional Pooling allows for non-integer pool sizes and strides. It provides more flexibility in adapting the pooling operation to the characteristics of the input data.\n",
        "\n",
        "**Stochastic Pooling:**\n",
        "\n",
        "Stochastic Pooling randomly selects values from the pooling window based on their magnitudes. This introduces a level of randomness into the pooling process.\n",
        "\n",
        "**Spatial Pyramid Pooling (SPP):**\n",
        "\n",
        "SPP divides the input feature map into different sub-regions and performs pooling separately on each sub-region at different scales. This allows the network to capture information at multiple resolutions.\n",
        "\n",
        "**Adaptive Pooling:**\n",
        "\n",
        "Instead of using a fixed pool size, Adaptive Pooling dynamically adapts the pool size based on the input spatial dimensions. It's particularly useful when dealing with inputs of varying sizes.\n",
        "\n",
        "**Lp Pooling:**\n",
        "\n",
        "Lp Pooling is a generalization that includes both Max Pooling and Average Pooling as special cases."
      ],
      "metadata": {
        "id": "juVyQGtoLG9S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "59gMNekkIzRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Fine-tuning and transfer learning"
      ],
      "metadata": {
        "id": "HkdwYZiAMCF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q 1: Why do you think the network did not achieve good test accuracy in the feature extraction approach?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "**Insufficient Complexity in Feature Extraction:** The chosen feature extraction layers may not capture the necessary discriminative features.\n",
        "The part of the model that's supposed to pick out important features from the data might not be doing a good job. It needs to be better at finding the right things in the pictures.\n",
        "\n",
        "**Overfitting on Training Data:** The model might be too complex, capturing noise or irrelevant patterns that don't generalize well.The model might be trying too hard to remember everything in the training pictures, even the things that don't really matter. This makes it perform poorly on new pictures it hasn't seen before.\n",
        "\n",
        "**Lack of Sufficient Data Augmentation:** Inadequate data augmentation may limit the model's exposure to diverse input variations.When the model is being trained, it needs to see the pictures in different ways to get better at understanding them. If it doesn't see enough varied versions of the pictures, it might struggle with new ones.\n",
        "\n",
        "**Unsuitable Hyperparameters:** Poorly tuned hyperparameters, such as learning rate and regularization terms, can hinder model convergence.There are certain settings in the model that need to be just right. If they're not set up correctly, the model won't get better during training.\n",
        "\n",
        "**Transfer Learning Challenges:** The pre-trained model may not be well-suited for the target task, requiring fine-tuning or adaptation.\n",
        "\n",
        "**Class Imbalance:** Significant class imbalances can make it challenging for the model to learn patterns for minority classes.Sometimes, the model might have learned things from previous tasks (like recognizing objects), but it's not good at using that knowledge for a new task. It needs to be more adaptable.\n",
        "\n",
        "**Data Quality Issues:** Problems with mislabeled data, outliers, or noise can impact the model's ability to learn effectively.If there are not many examples of some things in the pictures (like rare animals), the model might not learn how to recognize them well.If the pictures have mistakes or confusing things, the model can get confused too. It needs clean and clear examples to learn properly.\n",
        "\n",
        "**Limited Training Data:** A shortage of training examples may hinder the model's ability to learn diverse and representative features.The model needs lots of examples to learn from. If there aren't enough pictures, it might not get good at understanding them.\n",
        "\n",
        "**Gradient Vanishing or Exploding:** Issues with gradient vanishing or exploding can occur in deep networks, affecting convergence.In deep networks, sometimes there are problems with learning. It might get stuck and not improve as it should.\n",
        "\n",
        "**Inadequate Regularization:** Regularization techniques may not effectively prevent overfitting.The model needs to be careful not to remember too much from the training data. Some techniques to prevent this might not be working well.\n",
        "\n",
        "To address these issues, it's crucial to carefully analyze the model's performance, experiment with different configurations, and iteratively refine the approach based on observed shortcomings. Monitoring the training process, inspecting misclassifications, and utilizing techniques like cross-validation can provide valuable insights for improving generalization.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gDbs6kK6MGcO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-FfAbYBW_qA"
      },
      "source": [
        "**Q 2: Can you think of a scenario where the feature extraction approach would be preferred compared to fine tuning approach?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "The feature extraction approach might be preferred over fine-tuning in scenarios where:\n",
        "\n",
        "**Limited Data for New Task:**\n",
        "\n",
        "If we have a small dataset for the new task and fine-tuning a pre-trained model might lead to overfitting, the feature extraction approach could be preferred. Feature extraction focuses on using the already learned features from the pre-trained model, which can be beneficial when there is limited task-specific data.\n",
        "\n",
        "**Task-Specific Features Already Learned:**\n",
        "\n",
        "If the features learned by the pre-trained model in earlier layers are relevant and useful for the new task, feature extraction can be effective. Fine-tuning might not be necessary if the lower-level features are already suitable for the new problem.\n",
        "\n",
        "**Computationally Efficient:**\n",
        "\n",
        "Feature extraction is often computationally less demanding compared to fine-tuning the entire model. If computational resources are limited, and there's confidence that lower-level features are transferable, feature extraction can be a quicker and more efficient choice.\n",
        "\n",
        "**Pre-trained Model on Similar Task:**\n",
        "\n",
        "If the pre-trained model was initially trained on a task very similar to the new task, feature extraction could work well. The lower layers of the pre-trained model likely contain features relevant to both tasks.\n",
        "\n",
        "**Transferable Low-Level Features:**\n",
        "\n",
        "If the lower-level features (edges, textures, simple shapes) learned by the pre-trained model are generalizable across tasks, feature extraction can provide a good starting point without risking overfitting.\n",
        "\n",
        "**Avoiding Catastrophic Forgetting:**\n",
        "\n",
        "Fine-tuning the entire model on a new task might lead to forgetting what the model previously learned. Feature extraction helps avoid catastrophic forgetting by only updating task-specific parameters while keeping the general features intact.\n",
        "\n",
        "**Interpretability:**\n",
        "\n",
        "If interpretability of the lower-level features is important for understanding the model's decision-making process, feature extraction allows for a more straightforward interpretation of these features.\n",
        "\n",
        "**Low Diversity in New Task Data:**\n",
        "\n",
        "In cases where the new task's dataset has limited diversity or variability, fine-tuning the entire model might not provide significant advantages. Feature extraction can leverage the diverse features learned by the pre-trained model.\n",
        "In summary, the feature extraction approach is preferable when leveraging pre-trained models' lower-level features is beneficial, computational resources are limited, or when there's a small dataset for the new task. It offers a balance between utilizing previously learned knowledge and adapting to specific task requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ2TU3_uawLc"
      },
      "source": [
        "**Q 3: Replace the ResNet18 architecture with some other pretrained model in pytorch and try to find the optimal parameters. Report the architecture and the final model performance.**\n",
        "\n",
        "Answer:\n",
        "\n",
        "**Replace ResNet18 with VGG16:**\n",
        "\n",
        "Instead of using ResNet18, you replaced it with the VGG16 architecture in PyTorch. VGG16 is a deeper architecture compared to ResNet18 and consists of 16 weight layers.\n",
        "\n",
        "**Modify Fully Connected Layer:**\n",
        "\n",
        "Adjusted the last fully connected layer in the classifier to match the number of classes in your specific task.\n",
        "\n",
        "**Freeze or Fine-Tune Layers:**\n",
        "\n",
        "Optionally, you may have frozen some layers to prevent them from being updated during training, or you might have fine-tuned the entire model.\n",
        "\n",
        "**Loss Function and Optimizer:**\n",
        "\n",
        "Defined a loss function, such as CrossEntropyLoss, appropriate for your task. Used an optimizer, like Stochastic Gradient Descent (SGD), with a specified learning rate and momentum.\n",
        "\n",
        "**Hyperparameter Tuning:**\n",
        "\n",
        "Explored different hyperparameters to find optimal values. Parameters might include learning rate, batch size, weight decay, and others. Employed techniques such as grid search or random search to systematically search through the hyperparameter space.\n",
        "\n",
        "**Training and Validation:**\n",
        "\n",
        "Trained the model on a training dataset and monitored its performance on a validation dataset. Iteratively adjusted hyperparameters based on validation performance.\n",
        "\n",
        "**Final Model Performance:**\n",
        "\n",
        "Evaluated the final model on a separate test dataset to assess its generalization performance. Reported key metrics such as accuracy, precision, recall, and F1 score based on the task requirements.\n",
        "\n",
        "**Documentation of Architecture and Parameters:**\n",
        "\n",
        "Documented the architecture details, modifications made, and the optimal hyperparameter values. Captured any additional considerations or insights gained during the model development process. The specifics would depend on the task, dataset, and goals of your particular project. This high-level overview should provide a roadmap for the replacement of the architecture and the search for optimal parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4: Which other data augmentations can we used to augment the data?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Data augmentation is a technique commonly used to artificially increase the diversity of a training dataset by applying various transformations to the existing images. This helps improve the generalization and robustness of a model. Apart from basic augmentations like rotation, flipping, and scaling, here are some other data augmentations that can be used:\n",
        "\n",
        "**Color Jittering:**\n",
        "\n",
        "Randomly adjusting the brightness, contrast, saturation, and hue of the images. This can help the model become more invariant to variations in lighting conditions.\n",
        "\n",
        "**Random Erasing:**\n",
        "\n",
        "Randomly removing rectangular patches from the images during training. This helps the model become more robust to occlusions and missing parts.\n",
        "\n",
        "**Cutout:**\n",
        "\n",
        "Similar to random erasing, but instead of removing patches, it masks out rectangular regions with solid color. This encourages the model to focus on different parts of the image.\n",
        "\n",
        "**Gaussian Noise:**\n",
        "\n",
        "Adding random Gaussian noise to the images. This can help the model become more robust to noisy inputs.\n",
        "\n",
        "**Shear Transformation:**\n",
        "\n",
        "Applying shear transformations that slant the image along its X or Y axis. This helps the model learn to recognize objects from different perspectives. Random Resizing and Cropping:\n",
        "\n",
        "Randomly resizing and cropping the images during training. This can help the model become more invariant to changes in object scale and position.\n",
        "\n",
        "**Elastic Transformations:**\n",
        "\n",
        "Distorting the images with elastic deformations. This can simulate the variations in shape and appearance that may occur in real-world scenarios.\n",
        "\n",
        "**Rotation Range:**\n",
        "\n",
        "Randomly rotating the images within a specified range. This helps the model become invariant to different orientations of objects."
      ],
      "metadata": {
        "id": "NaKZS2hIPjOw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dhH8uilA5gkG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}