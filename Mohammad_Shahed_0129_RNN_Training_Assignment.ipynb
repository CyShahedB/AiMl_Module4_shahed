{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsayFhQ1F5AFf9pcf/kH94",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CyShahedB/AiMl_Module4_shahed/blob/main/Mohammad_Shahed_0129_RNN_Training_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jAOAIvmQ0Ew"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks with PyTorch"
      ],
      "metadata": {
        "id": "EebHW7nkQ4lt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f_mP2C5-MRE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " PyTorch library and its neural network module (nn), as well as the NumPy library (np). PyTorch is a popular deep learning library, and NumPy is commonly used for numerical operations in Python."
      ],
      "metadata": {
        "id": "7l3FDXs-Rq1x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UISxduQz-MRG"
      },
      "outputs": [],
      "source": [
        "text = ['hey how are you','good i am fine','have a nice day']\n",
        "\n",
        "# Join all the sentences together and extract the unique characters from the combined sentences\n",
        "chars = set(''.join(text))\n",
        "\n",
        "# Creating a dictionary that maps integers to the characters\n",
        "int2char = dict(enumerate(chars))\n",
        "\n",
        "# Creating another dictionary that maps characters to integers\n",
        "char2int = {char: ind for ind, char in int2char.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " we are working on encoding characters into integers and vice versa using dictionaries in Python. This is a common preprocessing step when working with text data in natural language processing tasks. Here, we are creating dictionaries to map characters to integers (char2int) and integers back to characters (int2char).\n",
        "\n",
        "The code you provided is well-structured. Just to clarify:\n",
        "\n",
        "1) chars contains the unique characters present in all the sentences.\n",
        "\n",
        "2) int2char is a dictionary that maps integers to characters.\n",
        "\n",
        "3) char2int is a dictionary that maps characters to integers.\n",
        "\n",
        "Now, we can use these dictionaries to convert your text data into numerical representations, which is often necessary when working with neural networks or other machine learning models."
      ],
      "metadata": {
        "id": "bZvm4tXXR60T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkKuM8KG-MRG",
        "outputId": "19febc52-0dc5-46ec-ed4b-db1c83ae7e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'v': 0, 'c': 1, 'o': 2, 'a': 3, 'f': 4, 'r': 5, 'u': 6, 'h': 7, 'n': 8, 'd': 9, 'w': 10, 'y': 11, 'g': 12, 'i': 13, 'm': 14, 'e': 15, ' ': 16}\n"
          ]
        }
      ],
      "source": [
        "print(char2int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " it will show the mapping of characters to integers that we have created."
      ],
      "metadata": {
        "id": "ALtbxHnoSWop"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIN_FpAc-MRJ",
        "outputId": "a76c76f6-fdab-416c-d727-e2e777c81232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The longest string has 15 characters\n"
          ]
        }
      ],
      "source": [
        "maxlen = len(max(text, key=len))\n",
        "print(\"The longest string has {} characters\".format(maxlen))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It calculates the length of the longest string in the text list using the max function and the len function. Then, it prints a message indicating the length of the longest string.\n",
        "\n",
        "In this case, the max function is used with the key=len argument to find the string with the maximum length in the text list. The len function is applied to each string, and the maximum length is determined.\n",
        "\n",
        "The result (maxlen) is then printed in a formatted string, indicating the length of the longest string in the text list.\n",
        "\n"
      ],
      "metadata": {
        "id": "egY6CnSsSiw5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Erncc0ul-MRK"
      },
      "outputs": [],
      "source": [
        "# Padding\n",
        "\n",
        "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of the sentence matches\n",
        "# the length of the longest sentence\n",
        "for i in range(len(text)):\n",
        "    while len(text[i])<maxlen:\n",
        "        text[i] += ' '"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " we are implementing padding to make all sentences in the text list have the same length. This is a common preprocessing step, especially when working with sequences like sentences in natural language processing. Padding ensures that all sequences are of equal length, which is often required when feeding data into neural networks.\n",
        "\n",
        "Our code snippet achieves this by looping through each sentence in the text list and adding whitespace (' ') to the end of each sentence until its length matches the length of the longest sentence (maxlen).\n",
        "\n",
        "After running this code, all sentences in the text list will have the same length, equal to maxlen. This is beneficial for creating uniform input data when working with neural networks or other machine learning models that expect fixed-size inputs."
      ],
      "metadata": {
        "id": "IgdlCW0DTBUn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDwJbZdB-MRM",
        "outputId": "430f80c0-df9b-4535-8939-641f0add9f69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: hey how are yo\n",
            "Target Sequence: ey how are you\n",
            "Input Sequence: good i am fine\n",
            "Target Sequence: ood i am fine \n",
            "Input Sequence: have a nice da\n",
            "Target Sequence: ave a nice day\n"
          ]
        }
      ],
      "source": [
        "# Creating lists that will hold our input and target sequences\n",
        "input_seq = []\n",
        "target_seq = []\n",
        "\n",
        "for i in range(len(text)):\n",
        "    # Remove last character for input sequence\n",
        "    input_seq.append(text[i][:-1])\n",
        "\n",
        "    # Remove firsts character for target sequence\n",
        "    target_seq.append(text[i][1:])\n",
        "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "you are creating input and target sequences for a character-level language modeling task. In this code snippet, you iterate through each sentence in the text list and create input sequences (input_seq) and target sequences (target_seq).\n",
        "\n",
        "input_seq: It contains sequences where the last character of each sentence in the text list is removed. This represents the input to your model.\n",
        "\n",
        "target_seq: It contains sequences where the first character of each sentence in the text list is removed. This represents the target output for your model, where the goal is to predict the next character in each sentence.\n",
        "\n",
        "The print statement is just for visualization, showing the corresponding input and target sequences for each sentence."
      ],
      "metadata": {
        "id": "CHjKmCUmTfkk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "154TIKlu-MRN"
      },
      "outputs": [],
      "source": [
        "for i in range(len(text)):\n",
        "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
        "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoxOt4re-MRN"
      },
      "source": [
        "Before encoding our input sequence into one-hot vectors, we'll define 3 key variables:\n",
        "\n",
        "- *dict_size*: The number of unique characters that we have in our text\n",
        "    - This will determine the one-hot vector size as each character will have an assigned index in that vector\n",
        "- *seq_len*: The length of the sequences that we're feeding into the model\n",
        "    - As we standardised the length of all our sentences to be equal to the longest sentences, this value will be the max length - 1 as we removed the last character input as well\n",
        "- *batch_size*: The number of sentences that we defined and are going to feed into the model as a batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L30rg5X9-MRO"
      },
      "outputs": [],
      "source": [
        "dict_size = len(char2int)\n",
        "seq_len = maxlen - 1\n",
        "batch_size = len(text)\n",
        "\n",
        "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
        "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
        "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
        "\n",
        "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
        "    for i in range(batch_size):\n",
        "        for u in range(seq_len):\n",
        "            features[i, u, sequence[i][u]] = 1\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we're defining a function one_hot_encode to convert our character sequences into one-hot encoded representations. One-hot encoding is a common technique used to represent categorical data, such as characters, in a binary matrix format.\n",
        "\n",
        "dict_size: The number of unique characters in our dataset.\n",
        "\n",
        "seq_len: The length of each sequence (excluding the last character for input sequences).\n",
        "\n",
        "batch_size: The number of sequences in our dataset.\n",
        "\n",
        "The function one_hot_encode takes a sequence as input and returns a one-hot encoded representation in the form of a multi-dimensional array (features). It iterates through each character in each sequence, setting the corresponding index in the one-hot encoded array to 1."
      ],
      "metadata": {
        "id": "KfP-cwoAUDW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6ajKdNl-MRO",
        "outputId": "fa5e9cb2-8b08-4f74-eb31-bb0565bf6da6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (3, 14, 17) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n"
          ]
        }
      ],
      "source": [
        "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
        "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " we are applying our one_hot_encode function to the input_seq to convert the character sequences into one-hot encoded representations."
      ],
      "metadata": {
        "id": "QotiIvzTUa0y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBGRENsr-MRO"
      },
      "source": [
        "Since we're done with all the data pre-processing, we can now move the data from numpy arrays to PyTorch's very own data structure - **Torch Tensors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKEAxoux-MRO"
      },
      "outputs": [],
      "source": [
        "input_seq = torch.from_numpy(input_seq)\n",
        "target_seq = torch.Tensor(target_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are converting your NumPy arrays (input_seq and target_seq) into PyTorch tensors using the torch.from_numpy() and torch.Tensor() functions, respectively.\n",
        "\n",
        "torch.from_numpy(input_seq): Converts the NumPy array input_seq to a PyTorch tensor.\n",
        "\n",
        "torch.Tensor(target_seq): Converts the NumPy array target_seq to a PyTorch tensor. Note that torch.Tensor() creates a new tensor with the same data type as the input."
      ],
      "metadata": {
        "id": "Lt7l1CTMVIq4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbA19w5_-MRP",
        "outputId": "8cef3fef-e9e7-413b-fc98-34de389fe9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU not available, CPU used\n"
          ]
        }
      ],
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code checks if a GPU is available using torch.cuda.is_available() and sets the device variable accordingly. If a GPU is available, it sets device to \"cuda\", and if not, it sets it to \"cpu\".\n",
        "\n",
        "torch.cuda.is_available(): Checks if a GPU is available and returns a boolean value (True if GPU is available, False otherwise).\n",
        "\n",
        "The code then uses an if statement to determine whether a GPU is available. If a GPU is available, it sets the device variable to \"cuda\" and prints a corresponding message. If no GPU is available, it sets the device variable to \"cpu\" and prints a message indicating that the CPU will be used.\n",
        "\n",
        "This is a common practice to dynamically set the device based on GPU availability, allowing for flexibility in running code on either a GPU or CPU."
      ],
      "metadata": {
        "id": "nDbAQ8_dVkRR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2kKqH8--MRP"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        #Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "\n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a simple RNN (Recurrent Neural Network) model using PyTorch's nn.Module class. The model consists of an RNN layer and a fully connected layer. The forward method specifies how input data flows through the layers, and there's an additional method init_hidden to initialize the hidden state."
      ],
      "metadata": {
        "id": "f2AjzKmbV5Ag"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI6d1o3N-MRP"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model with hyperparameters\n",
        "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 100\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can begin our training! As we only have a few sentences, this training process is very fast. However, as we progress, larger datasets and deeper models mean that the input data is much larger and the number of parameters within the model that we have to compute is much more."
      ],
      "metadata": {
        "id": "TK9O06xfWA5n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztyx2Elt-MRQ",
        "outputId": "b93fac2b-0a92-42ff-ec47-0e57c34c1606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/100............. Loss: 2.4408\n",
            "Epoch: 20/100............. Loss: 2.0811\n",
            "Epoch: 30/100............. Loss: 1.6645\n",
            "Epoch: 40/100............. Loss: 1.2433\n",
            "Epoch: 50/100............. Loss: 0.8861\n",
            "Epoch: 60/100............. Loss: 0.6095\n",
            "Epoch: 70/100............. Loss: 0.4061\n",
            "Epoch: 80/100............. Loss: 0.2775\n",
            "Epoch: 90/100............. Loss: 0.1993\n",
            "Epoch: 100/100............. Loss: 0.1517\n"
          ]
        }
      ],
      "source": [
        "# Training Run\n",
        "input_seq = input_seq.to(device)\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "    #input_seq = input_seq.to(device)\n",
        "    output, hidden = model(input_seq)\n",
        "    output = output.to(device)\n",
        "    target_seq = target_seq.to(device)\n",
        "    loss = criterion(output, target_seq.view(-1).long())\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "\n",
        "    if epoch%10 == 0:\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Loss: {:.4f}\".format(loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUW_aRxs-MRQ"
      },
      "source": [
        "Let’s test our model now and see what kind of output we will get. Before that, let’s define some helper function to convert our model output back to text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4VpkpkJ-MRQ"
      },
      "outputs": [],
      "source": [
        "def predict(model, character):\n",
        "    # One-hot encoding our input to fit into the model\n",
        "    character = np.array([[char2int[c] for c in character]])\n",
        "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
        "    character = torch.from_numpy(character)\n",
        "    character = character.to(device)\n",
        "\n",
        "    out, hidden = model(character)\n",
        "\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "    # Taking the class with the highest probability score from the output\n",
        "    char_ind = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "    return int2char[char_ind], hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predict function takes a trained model and a single character as input and returns the predicted next character and the hidden state.\n",
        "\n",
        "The function first converts the input character into a one-hot encoded representation suitable for the model.\n",
        "\n",
        "Then, it performs a forward pass through the model to obtain the output and hidden state.\n",
        "\n",
        "The output is passed through a softmax function to obtain probabilities for each character.\n",
        "\n",
        "The index of the character with the highest probability is extracted (char_ind).\n",
        "\n",
        "Finally, the predicted character and the hidden state are returned.\n",
        "\n",
        "This function can be used for generating sequences character by character using the trained RNN model."
      ],
      "metadata": {
        "id": "fIQKYGsuWTr9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p0TGaID-MRR"
      },
      "outputs": [],
      "source": [
        "def sample(model, out_len, start='hey'):\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    # First off, run through the starting characters\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    # Now pass in the previous characters and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(model, chars)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sample function generates a sequence of characters given a starting string (start). It uses the trained model to predict the next characters in the sequence."
      ],
      "metadata": {
        "id": "mS3WXTS5Wsba"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lu_sonwz-MRR",
        "outputId": "cee4f872-431e-4102-9248-63ccd8a2f219"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good i am fine '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "sample(model, 15, 'good')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw33Is9w-MRR"
      },
      "source": [
        "As we can see, the model is able to come up with the sentence ‘good i am fine ‘ if we feed it with the words ‘good’, achieving what we intended for it to do!"
      ]
    }
  ]
}