{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CyShahedB/AiMl_Module4_shahed/blob/main/Training_a_Neural_Network_Mohammad_Shahed_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notebook Created by: Abdul Adhil P K\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# What is a Neural Network\n",
        "\n",
        "\n",
        "\n",
        "Neural networks are a set of algorithms inspired by the functioning of the human brain. When you open your eyes, the information you perceive, known as data, is processed by neurons, which are the data processing cells in your brain. These neurons recognize patterns in the data and enable you to identify and understand your surroundings.\n",
        "\n",
        "Here is an example of Neural Network trying to predict the image data that given to it(the image data is 28x28 pixels, thats 784 pixels at input neurons). it predicts that the no is 2 here:\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:640/format:webp/0*aWIO7eB6E4-cIkK9.gif)\n",
        "\n"
      ],
      "metadata": {
        "id": "TVPFp2Bx-BJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Breaking Down The Neural Network!\n",
        "\n",
        "\n",
        "### 1. Data\n",
        "  The type of data a neural network processes varies drastically based on the problem being solved. When we build a neural network, we define what shape and kind of data it can accept. It may sometimes be neccessary to modify our dataset so that it can be passed to our neural network.\n",
        "\n",
        "### 2. Layers\n",
        "  As we mentioned earlier each neural network consists of multiple layers. At each layer a different transformation of data occurs. Our initial input data is fed through the layers and eventually arrives at the output layer where we will obtain the result.\n",
        "\n",
        "  * Input Layer:\n",
        "  The input layer is the layer that our initial data is passed to. It is the first layer in our neural network.\n",
        "\n",
        "  * Output Layer:\n",
        "  The output layer is the layer that we will retrive our results from. Once the data has passed through all other layers it will arrive here.\n",
        "\n",
        "  * Hidden Layer(s):\n",
        "  All the other layers in our neural network are called \"hidden layers\". This is because they are hidden to us, we cannot observe them. Most neural networks consist of at least one hidden layer but can have an unlimited amount. Typically, the more complex the model the more hidden layers.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:640/format:webp/0*BSxP3AHxBe_IevHC.png)\n",
        "\n",
        "### 3. Neurons\n",
        "\n",
        "Each layer is made up of what are called neurons. For example, say we want to pass an image that is 28x28 pixels, thats 784 pixels. We would need 784 neurons in our input layer to capture each of these pixels.\n",
        "\n",
        "### 4. Weights\n",
        "  Weights are associated with each connection in our neural network. Every pair of connected nodes will have one weight that denotes the strength of the connection between them. The model will try to determine what these weights should be to achieve the best result. Weights start out at a constant or random value and will change as the network sees training data.\n",
        "\n",
        "### 5. Biases\n",
        "\n",
        "  A bias is simply a constant value associated with each layer. It can be thought of as an extra neuron that has no connections. The purpose of a bias is to shift an entire activation function by a constant value. This allows a lot more flexibllity when it comes to choosing an activation and training the network. There is one bias for each layer.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:960/1*0lejoYyyQWjYzEP_BNW2nw.jpeg)\n",
        "\n",
        "### 6. Activation Function\n",
        "\n",
        "Activation functions are simply a function that is applied to the weighed sum of a neuron. They can be anything we want but are typically higher order/degree functions that aim to add a higher dimension to our data. We would want to do this to introduce more complexity to our model.\n",
        "\n",
        "\n",
        "A list of some common activation functions and their graphs can be seen below:\n",
        "\n",
        "* #### Sigmoid function:\n",
        "\n",
        "  Transform $ (- \\infty $ to $ \\infty) $ into (0 to 1) range\n",
        "  $$\n",
        "  sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
        "  $$\n",
        "\n",
        "* #### Tanh function:\n",
        "\n",
        "  Similar to sigmoid, difference being that output is -1 to +1\n",
        "\n",
        "\n",
        "$$\n",
        "tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "$$\n",
        "\n",
        "  \n",
        "\n",
        "* #### ReLU\n",
        "\n",
        "  $$\n",
        "  f(x) = \\begin{cases} %\n",
        "                      0 & if \\; x<0 \\\\\n",
        "                      x &  if \\;x \\geq 0.\n",
        "                  \\end{cases}\n",
        "  $$\n",
        "\n",
        "![](https://www.researchgate.net/publication/327435257/figure/fig4/AS:742898131812354@1554132125449/Activation-Functions-ReLU-Tanh-Sigmoid.ppm)\n",
        "\n",
        "There are several other loss functions, each with its own specific use cases and characteristics, you can explore those at your own pace."
      ],
      "metadata": {
        "id": "KR9hR8ak7hgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## How it Works\n",
        "\n",
        "A neural network consists of many Nodes (Neurons) in many layers. Each layer can have any number of nodes and a neural network can have any number of layers.\n",
        "\n",
        "\n",
        "\n",
        "Lets take the example of whats going on with a single node in the network.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*SaQMHTLi4C7MIA4IzjAXJw.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "Y =(\\sum_{i=0}^n w_i x_i) + b\n",
        "\\end{equation}\n",
        "\n",
        " * w: stands for the weight of each connection to the neuron\n",
        "\n",
        " * x: stands for the value of the connected neuron from the previous value\n",
        "\n",
        " * b: stands for the bias at each layer, this is a constant\n",
        "\n",
        " * n: is the number of connections\n",
        "\n",
        " * Y: is the output of the current neuron\n",
        "\n",
        "\n",
        " The equation you just read is called a weighed sum. We will take this weighted sum at each and every neuron as we pass information through the network. Then we will add what's called a bias to this sum. The bias allows us to shift the network up or down by a constant value. It is like the y-intercept of a line.\n",
        "\n",
        "\n",
        " But that equation is the not complete one! We forgot a crucial part, the **activation function**. Our new equation with the addition of an activation function\n",
        " is seen below.\n",
        "\n",
        "\\begin{equation}\n",
        " Y =F((\\sum_{i=0}^n w_i x_i) + b)\n",
        "\\end{equation}\n",
        "\n"
      ],
      "metadata": {
        "id": "0dIWWXD45sHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Every Neural Network has 2 main parts:\n",
        "\n",
        "* Forward Propagation.\n",
        "* Backward Propagation.\n",
        "\n",
        "![](https://www.researchgate.net/publication/327637282/figure/fig1/AS:670566579175436@1536886939536/A-simple-neural-network-with-two-hidden-layers-of-two-nodes-each-four-inputs-and-a.ppm)\n",
        "\n",
        "# Forward Propogation.\n",
        "\n",
        "* Forward propagation is the process in which input data is processed through the neural network's layers to produce an output.\n",
        "\n",
        "* It involves passing the input data through each layer of the neural network, applying the layer's weights and activation functions, until the final output is obtained.\n",
        "\n",
        "# Backpropagation\n",
        "Backpropagation is the fundemental algorithm behind training neural networks. It is what changes the weights and biases of our network. To fully explain this process, we need to learn something called a cost/loss function.\n",
        "\n",
        "# Loss/Cost Function\n",
        "\n",
        "For our training data we have the features (input) and the labels (expected output), because of this we can compare the output from our network to the expected output. Based on the difference between these values we can determine if our network has done a good job or poor job. If the network has done a good job, we'll make minor changes to the weights and biases. If it has done a poor job our changes may be more drastic.\n",
        "\n",
        "Some common loss/cost functions include.\n",
        "\n",
        "* Mean Squared Error\n",
        "\\begin{equation}\n",
        "y = \\sum_{i=1}^{D}(x_i-y_i)^2\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "* Mean Absolute Error\n",
        "\\begin{equation}y = \\sum_{i=1}^{D}|x_i-y_i|\n",
        "\\end{equation}\n",
        "* Hinge Loss\n",
        "\\begin{equation}\n",
        "y = max(0, 1 - y \\cdot \\hat{y})\n",
        "\\end{equation}\n",
        "\n",
        "Where 'D' represents the number of samples in the dataset.\n",
        "\n",
        "# Optimizer\n",
        "Optimization function is simply the function that implements the backpropagation algorithm described above. Here's a list of a few common ones.\n",
        "\n",
        "* Gradient Descent\n",
        "* Stochastic Gradient Descent\n",
        "* Mini-Batch Gradient Descent\n",
        "* Momentum\n",
        "* Nesterov Accelerated Gradient\n",
        "\n",
        "![](https://i.pinimg.com/originals/6f/d6/22/6fd62253592b42795c48dc570a17579c.gif)\n",
        "\n",
        "During backpropagation we calculate the total error at the output nodes and propagate these errors back through the network using Backpropagation to calculate the gradients. Then we use an optimization method such as Gradient Descent to adjust all weights in the network with an aim of reducing the error at the output layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "gPpxXw9S-HZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Nueral Network"
      ],
      "metadata": {
        "id": "UbEFZXhaMLHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "import random"
      ],
      "metadata": {
        "id": "83mCQD8QFPFH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "\n",
        "iris = load_iris()\n",
        "data, labels = iris.data[:,0:2], iris.data[:,2]\n",
        "\n",
        "num_samples = len(labels)  # size of our dataset\n",
        "\n",
        "# shuffle the dataset\n",
        "shuffle_order = np.random.permutation(num_samples)\n",
        "data = data[shuffle_order, :]\n",
        "labels = labels[shuffle_order]"
      ],
      "metadata": {
        "id": "VK2wHTQ1FHlB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---->This code loads the Iris dataset, selects specific columns for features and labels, calculates the number of samples, and then shuffles the dataset to ensure randomization.\n"
      ],
      "metadata": {
        "id": "7F7ho-QDet6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like the 1-dimensional problem previously, we can still do linear regression, except now we have two variables and therefore two weights as well. Let's denote the input variables as x1 and x2 and instead of using m as the coefficient variable, let's use w1 and w2. So for linear regression, we would have the following function:\n",
        "\n",
        "$$\n",
        "f(X) = w_1 x_1 + w_2 x_2 + b\n",
        "$$\n",
        "\n",
        "\n",
        "For example, suppose set w = [0.2, 0.6] and b = -0.3. Let's calculate the resulting . We can program this as a function called \"weighted_sum\"."
      ],
      "metadata": {
        "id": "1JFlz0COF9Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_sum(x, w, b):\n",
        "    return b + np.dot(w, x)\n",
        "\n",
        "# set our paramters - weights and bias\n",
        "w = [0.2, 0.6]\n",
        "b = -0.3\n",
        "\n",
        "# for example, let's use the first data point\n",
        "X, y = data, labels\n",
        "\n",
        "pred_y = [weighted_sum(x, w, b) for x in X]\n",
        "\n",
        "# let's print out the first prediction\n",
        "print(\"for x=[%0.2f, %0.2f], predicted = %0.2f, actual = %0.2f\" % (X[0][0], X[0][1], pred_y[0], y[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u5YRiIvFctw",
        "outputId": "f5ece696-70f5-45cd-8774-26a9cf98feac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for x=[5.80, 4.00], predicted = 3.26, actual = 1.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-->This code defines a simple linear model, initializes weights and bias, makes predictions for each data point in the dataset, and prints the prediction and actual value for the first data point.\n"
      ],
      "metadata": {
        "id": "7UbsZ7F7fHNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can evaluate the quality of our predictions using cost/loss functions. Lets use the sum-squared error function\n",
        "\n"
      ],
      "metadata": {
        "id": "Ay4a6YMhG6tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sum squared error\n",
        "def cost_function(y_pred, y_actual):\n",
        "    return 0.5 * np.sum((y_actual-y_pred)**2)\n",
        "\n",
        "error = cost_function(pred_y, y)\n",
        "print(error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hysg_sq0FoKA",
        "outputId": "e74a56bb-014a-4d0d-b57a-6731592fa685"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313.50559999999996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---->By using MSE(Mean Squared Error) we are calculating the error in predicted output"
      ],
      "metadata": {
        "id": "iJEznApakgmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing the data\n",
        "X = X / np.amax(X, axis=0)\n",
        "y = y / np.amax(y, axis=0)\n",
        "\n",
        "# randomly initializing w, b\n",
        "w, b = [random.random(), random.random()], random.random()\n",
        "\n",
        "# our function w*x + b\n",
        "def F(X, w, b):\n",
        "    return np.sum(w*X, axis=1) + b\n",
        "\n",
        "# calculating error using cost function(Here we use Mean Squared Error)\n",
        "y_pred = F(X, w, b)\n",
        "init_cost = cost_function(y_pred, y)\n",
        "\n",
        "print(\"initial parameters: w1=%0.3f, w2=%0.3f, b=%0.3f\"%(w[0], w[1], b))\n",
        "print(\"initial cost = %0.3f\" % init_cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuZqQYNmHFUf",
        "outputId": "a5edabf3-1811-4877-e80d-654a684b4d60"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial parameters: w1=0.658, w2=0.968, b=0.564\n",
            "initial cost = 109.281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---->This code snippet normalizes the data, initializes weights and bias randomly, defines a linear model function, calculates initial predictions, and prints the initial parameters and cost."
      ],
      "metadata": {
        "id": "iU5GgxZplgXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the partial derivatives are:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_1} = - \\sum{x_1^i \\cdot (y^i - (w_1 x_1^i + w_2 x_2^i+ b))}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_2} = - \\sum{x_2^i \\cdot (y^i - (w_1 x_1^i + w_2 x_2^i+ b))}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b} = - \\sum{y^i - (w_1 x_1^i + w_2 x_2^i+ b)}\n",
        "$$"
      ],
      "metadata": {
        "id": "kpR3WeBjJ2sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# implement partial derivatives of our parameters\n",
        "\n",
        "def dJdw1(X, y, w, b):\n",
        "    return -np.dot(X[:,0], y - F(X, w, b))\n",
        "\n",
        "def dJdw2(X, y, w, b):\n",
        "    return -np.dot(X[:,1], y - F(X, w, b))\n",
        "\n",
        "def dJdb(X, y, w, b):\n",
        "    return -np.sum(y - F(X, w, b))"
      ],
      "metadata": {
        "id": "CveoHZvHJYPO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---->These partial derivatives are used to update the parameters (w1, w2, and b) iteratively in the opposite direction of the gradient, aiming to minimize the cost function.\n",
        "\n",
        "---->Here, learning_rate is a hyperparameter controlling the step size of the updates in the gradient descent process. The negative sign ensures that the parameters are updated in the direction that reduces the cost function."
      ],
      "metadata": {
        "id": "rbjaVIjvmO55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aftet that, we use the following update rule, where we calculate the gradient and then adjust the parameters.\n",
        "\n",
        "$$\n",
        "w_1 = w_1 - \\alpha \\cdot \\frac{\\partial J}{\\partial w_i}\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_2 = w_2 - \\alpha \\cdot \\frac{\\partial J}{\\partial w_2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b = b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
        "$$"
      ],
      "metadata": {
        "id": "KEAv94RcKQGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# choose the learning rate parameter and number of iterations\n",
        "lr = 0.001\n",
        "n_iters = 2000\n",
        "\n",
        "# run through gradient descent\n",
        "errors = []\n",
        "for i in range(n_iters):\n",
        "    w[0] = w[0] - lr * dJdw1(X, y, w, b)\n",
        "    w[1] = w[1] - lr * dJdw2(X, y, w, b)\n",
        "    b = b - lr * dJdb(X, y, w, b)\n",
        "    y_pred = F(X, w, b)\n",
        "    j = cost_function(y_pred, y)\n",
        "    errors.append(j)"
      ],
      "metadata": {
        "id": "RhZBXPFyJksq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the error\n",
        "plt.plot(range(n_iters), errors, linewidth=2)\n",
        "plt.title(\"Cost by iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.xlabel(\"iterations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "Yj9y6jspLAVp",
        "outputId": "1a42a46f-3753-428a-9527-4b6087bddffb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'iterations')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----> Here we are plotting iterations with cost, as iteration increases the cost is decreasing.\n",
        "* At the beginning of training, the loss is typically high as the model's weights are randomly initialized,the model struggles to make accurate predictions, resulting in a high training loss.\n",
        "\n",
        "* As the training progresses, the loss generally decreases, the model adjusts its weights and biases to minimize the difference between its predictions and the actual target values."
      ],
      "metadata": {
        "id": "bQKAdGigLwoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's implement the sigmoid function.\n",
        "\n"
      ],
      "metadata": {
        "id": "zGyf_gw2NMrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))"
      ],
      "metadata": {
        "id": "soOMzv5qLEEI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically, a sigmoid just squashes any input it gets to between 0 and 1. So we now modify our basic function in the following way: instead of just outputting the weighted sum by itself,we now pass that through the sigmoid function.\n",
        "\n",
        "  So instead of y = (w_1 * x_1) + (w_2 * x_2) + b our function now looks like:\n",
        "\n",
        "$$\n",
        "y = \\frac{1}{1 + e^{-(w_1 x_1 + w_2 x_2 + b)}}\n",
        "$$"
      ],
      "metadata": {
        "id": "CtafACY5NYq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_sum(x, w, b):\n",
        "    return b + np.dot(w, x)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# our parameters\n",
        "w = [0.2, 0.6]\n",
        "b = -0.3\n",
        "\n",
        "X, y = data, labels\n",
        "\n",
        "# get weighted sum like before\n",
        "Z = [weighted_sum(x, w, b) for x in X]\n",
        "\n",
        "# now transform the weighted sums with a sigmoid\n",
        "y_pred = [sigmoid(z) for z in Z]\n",
        "\n",
        "# evaluate error\n",
        "error = cost_function(y_pred, y)\n",
        "print(error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXQ3uzNLOCah",
        "outputId": "0edacb25-5ee6-4dfe-a2fd-59b0531ad063"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "829.3581552057614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---->This code implements a logistic regression model by calculating the weighted sums, applying the sigmoid activation function, and evaluating the error using a cost function.\n",
        "\n",
        "---->The logistic regression model is commonly used for binary classification problems where the goal is to predict probabilities of belonging to a particular class."
      ],
      "metadata": {
        "id": "s3edCirsnsHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function we posed above can already be considered a neural network. But let's complicate things a bit further, by adding a hidden layer. Neurons can be arranged in layers. So instead of having just two input neurons and an output neuron, let's place a layer of three neurons in the middle\n",
        "\n",
        "![](https://raw.githubusercontent.com/ml4a/ml4a/a8831f15b581f091d16003b0b61a68ed1bbbb770/assets/neuralnet.jpg)"
      ],
      "metadata": {
        "id": "yTsbzEwKOvQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = np.random.randn(2, 3)\n",
        "W2 = np.random.randn(3, 1)\n",
        "\n",
        "print(\"W1=\", W1)\n",
        "print(\"W2=\", W2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-MoMt2YOkTL",
        "outputId": "c900273b-a8cb-498b-8402-b857785bf3ae"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1= [[ 0.80349708  0.48546117 -2.4009011 ]\n",
            " [ 0.65692168  0.57410392 -0.2280385 ]]\n",
            "W2= [[0.43399687]\n",
            " [0.1950419 ]\n",
            " [1.25050896]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---->Here we are taking weights randomly."
      ],
      "metadata": {
        "id": "ZFFYTf34n4rk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### implementing a forward pass"
      ],
      "metadata": {
        "id": "cVWOhKp9PFM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X, y = data, labels\n",
        "\n",
        "# first layer weighted sum z\n",
        "z = np.dot(X, W1)\n",
        "\n",
        "# project z through non-linear sigmoid\n",
        "z = sigmoid(z)\n",
        "\n",
        "# do another dot product at end (sigmoid is omitted)\n",
        "y_pred = np.dot(z, W2)\n",
        "\n",
        "# what is our cost\n",
        "error = cost_function(y_pred, y)\n",
        "\n",
        "print('predicted %0.2f for example 0, actual %0.2f, total cost %0.2f'%(pred_y[0], y[0], error))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vaPe9UeO9m7",
        "outputId": "e2fa398c-8d5f-48bb-8766-fb825832a989"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted 3.26 for example 0, actual 1.20, total cost 145164.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---->This code represents a simple neural network with one hidden layer. It performs a linear transformation on the input features, applies a sigmoid activation function to introduce non-linearity, and then makes a final prediction through another linear transformation.\n",
        "\n",
        "---->The error is then calculated using a cost function, and the results are printed. The code snippet is missing the definitions for W1, W2, sigmoid, and cost_function, which should be defined elsewhere in our code."
      ],
      "metadata": {
        "id": "yZkLGZDQokAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's turn the above operations into a class.\n",
        "\n"
      ],
      "metadata": {
        "id": "J8iY5h-4PYpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neural_Network(object):\n",
        "    def __init__(self, n0, n1, n2):\n",
        "        self.n0 = n0\n",
        "        self.n1 = n1\n",
        "        self.n2 = n2\n",
        "\n",
        "        # initialize weights\n",
        "        self.W1 = np.random.randn(self.n0, self.n1)\n",
        "        self.W2 = np.random.randn(self.n1 ,self.n2)\n",
        "\n",
        "    def predict(self, x):\n",
        "        z = np.dot(x, self.W1)\n",
        "        z = sigmoid(z)\n",
        "        y = np.dot(z, self.W2)\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "6F2SHwt0PDIW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate a neural network with 2 input neurons, 3 hidden neurons, and 1 output neuron using above class\n",
        "\n",
        "net = Neural_Network(2, 3, 1)"
      ],
      "metadata": {
        "id": "il41fIVZPSBb"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---->This code defines a simple neural network class with a constructor for initializing the weights and a predict method for making predictions based on the current weights.\n",
        "\n",
        "---->The neural network has two layers: an input layer with n0 nodes, a hidden layer with n1 nodes, and an output layer with n2 nodes. The weights are initialized randomly using the standard normal distribution."
      ],
      "metadata": {
        "id": "OnDWur77pBya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to do a forward pass, we can simply run the networks predict function:\n"
      ],
      "metadata": {
        "id": "rxCMltIMPwST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = data, labels\n",
        "y_pred = net.predict(X)\n",
        "error = cost_function(y_pred, y)\n",
        "\n",
        "print('predicted %0.2f for example 0, actual %0.2f, total cost %0.2f'%(pred_y[0], y[0], error))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P87xH2HPrWt",
        "outputId": "29003ea0-bd06-4118-a0b4-753d52c5fd66"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted 3.26 for example 0, actual 1.20, total cost 223425.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---->This code uses the neural network (net) that was previously defined to make predictions on the dataset (data) and calculates the error using a cost function."
      ],
      "metadata": {
        "id": "jTNsRmBLpi3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " we have a 2x3x1 neural network with 9 weights and 4 biases for 13 total parameters.\n",
        "\n",
        " Now we optimize the parameters to minimize our cost function using Gradient Descent.\n",
        "\n",
        "Gradient Descent will find the gradient of the cost/loss function(J)\n",
        " with respect to the parameters w,b.\n",
        "\n",
        " $$\n",
        " w_i := w_i - \\alpha \\cdot \\frac{\\partial J}{\\partial w_i}\n",
        " $$"
      ],
      "metadata": {
        "id": "mDMiAPzJQHQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gradient(net, X, y):\n",
        "    w_delta = 1e-8\n",
        "\n",
        "    # get the current value of the loss, wherever the parameters are\n",
        "    y_pred_current = net.predict(X)\n",
        "    error_current = cost_function(y_pred_current, y)\n",
        "\n",
        "    # grab the current weights and copy them (so we can restore them after modification)\n",
        "    dw1, dw2 = np.zeros((net.n0, net.n1)), np.zeros((net.n1, net.n2))\n",
        "    W1, W2 = np.copy(net.W1), np.copy(net.W2)\n",
        "\n",
        "    # Calculate gradient for the first layer\n",
        "    for i in range(net.n0):\n",
        "        for j in range(net.n1):\n",
        "            net.W1 = np.copy(W1)\n",
        "            net.W1[i][j] += w_delta\n",
        "            y_pred = net.predict(X)\n",
        "            error = cost_function(y_pred, y)\n",
        "            dw1[i][j] = (error - error_current) / w_delta\n",
        "\n",
        "    # Calculate gradient for the second layer\n",
        "    for i in range(net.n1):\n",
        "        for j in range(net.n2):\n",
        "            net.W2 = np.copy(W2)\n",
        "            net.W2[i][j] += w_delta\n",
        "            y_pred = net.predict(X)\n",
        "            error = cost_function(y_pred, y)\n",
        "            dw2[i][j] = (error - error_current) / w_delta\n",
        "\n",
        "    # restore the original weights\n",
        "    net.W1, net.W2 = np.copy(W1), np.copy(W2)\n",
        "\n",
        "    return dw1, dw2"
      ],
      "metadata": {
        "id": "eXEVFmcoP05N"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---->This method is an implementation of numerical gradient checking, a technique to verify the correctness of the backpropagation algorithm by comparing its gradients with numerically computed gradients.\n",
        "\n",
        "---->It's often used for debugging and validating the correctness of the gradient computation in neural network implementations."
      ],
      "metadata": {
        "id": "fnDdsQGyplh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above function 'get_gradient' calculates the gradient of a 2-layer network net, for our dataset X, y"
      ],
      "metadata": {
        "id": "ZUNZoWSBSfUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train the network by the following steps:\n",
        "\n",
        "1. Load our dataset\n",
        "2. Instantiate a neural network\n",
        "3. Train it on the data using the gradient method made above."
      ],
      "metadata": {
        "id": "5U_VzLU4Sp_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data and labels\n",
        "X, y = data, labels.reshape((len(labels),1))\n",
        "\n",
        "# it's always a good idea to normalize the data between 0 and 1\n",
        "X = X/np.amax(X, axis=0)\n",
        "y = y/np.amax(y, axis=0)\n",
        "\n",
        "# create a 2x3x1 neural net\n",
        "net = Neural_Network(2, 3, 1)\n",
        "\n",
        "# what is the current cost?\n",
        "y_orig = net.predict(X)\n",
        "init_cost = cost_function(y_orig, y)\n",
        "print(\"initial cost = %0.3f\" % init_cost)\n",
        "\n",
        "# Set the learning rate, and how many epochs (updates) to try\n",
        "n_epochs = 2000\n",
        "learning_rate = 0.01\n",
        "\n",
        "# for each epoch, calculate the gradient, then subtract it from the parameters, and save the cost\n",
        "errors = []\n",
        "for i in range(n_epochs):\n",
        "    dw1, dw2 = get_gradient(net, X, y)\n",
        "    net.W1 = net.W1 - learning_rate * dw1\n",
        "    net.W2 = net.W2 - learning_rate * dw2\n",
        "    y_pred = net.predict(X)\n",
        "    error = cost_function(y_pred, y)\n",
        "    errors.append(error)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw5Q_cWWSY45",
        "outputId": "2a0c403d-0e04-4498-be5a-ea9a9d2b7ee9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial cost = 28.975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---->This code essentially trains a neural network using gradient descent by iteratively updating the weights based on the computed gradients. The goal is to reduce the cost function, making the predictions of the neural network closer to the actual labels. The evolution of the cost over epochs is stored in the errors list for analysis."
      ],
      "metadata": {
        "id": "ExHQFHJsqA5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting iterations vs error\n",
        "plt.plot(range(0, len(errors)), errors)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Error')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "YfzAFsVoTHgH",
        "outputId": "4a93222c-bcb1-4010-fa06-ee6405e1023a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Error')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---->The code is generating a plot that visualizes how the error changes over the course of the training iterations.\n",
        "\n",
        "---->This type of plot is commonly used in machine learning to assess the convergence of the training process and to understand how well the model is learning from the data.\n",
        "\n",
        "---->Ideally, the error should decrease over iterations, indicating that the model is improving its performance on the training data."
      ],
      "metadata": {
        "id": "ASCGv929qItp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xg_JZcC8qcBV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qfa0xAlcqrEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q:What are the advantages of neural networks and its applications?**"
      ],
      "metadata": {
        "id": "O7KdEQigqrI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks have gained widespread popularity and find applications across various domains due to their ability to learn complex patterns from data. Here are some advantages of neural networks and their diverse applications:\n",
        "\n",
        "#**Advantages:**\n",
        "\n",
        "**Ability to Learn Complex Patterns:**\n",
        "\n",
        "Neural networks can model and learn intricate relationships and patterns in data, making them suitable for tasks involving non-linearity and complexity.\n",
        "\n",
        "**Adaptability:**\n",
        "\n",
        "Neural networks are adaptable and can learn from experience. They can adjust their internal parameters to improve performance on a specific task over time.\n",
        "\n",
        "**Parallel Processing:**\n",
        "\n",
        "Neural networks are inherently parallel systems, enabling them to process multiple inputs simultaneously. This can lead to faster training and prediction times, especially for tasks involving large datasets.\n",
        "\n",
        "**Feature Learning:**\n",
        "\n",
        "Neural networks can automatically learn relevant features from raw data, reducing the need for manual feature engineering.\n",
        "\n",
        "**Generalization:**\n",
        "\n",
        "Well-designed neural networks can generalize well to unseen data, making them effective in making predictions on new, previously unseen examples.\n",
        "\n",
        "**Robustness to Noise:**\n",
        "\n",
        "Neural networks can exhibit robustness to noisy data, making them effective in real-world scenarios where data may be imperfect or incomplete.\n",
        "\n",
        "**Scalability:**\n",
        "\n",
        "Neural networks can scale to handle large amounts of data, and with advancements in hardware and distributed computing, they can be trained on massive datasets.\n",
        "\n",
        "#**Applications:**\n",
        "\n",
        "**Image and Speech Recognition:**\n",
        "\n",
        "Convolutional Neural Networks (CNNs) are widely used for image recognition, object detection, and facial recognition. Recurrent Neural Networks (RNNs) are applied in speech recognition tasks.\n",
        "\n",
        "**Natural Language Processing (NLP):**\n",
        "\n",
        "Neural networks, especially Recurrent and Transformer models, have revolutionized NLP tasks, including machine translation, sentiment analysis, and text summarization.\n",
        "\n",
        "**Autonomous Vehicles:**\n",
        "\n",
        "Neural networks play a crucial role in enabling self-driving cars by processing sensor data, recognizing objects, and making real-time decisions.\n",
        "\n",
        "**Healthcare:**\n",
        "\n",
        "Neural networks are used in medical image analysis, disease diagnosis, drug discovery, and personalized medicine.\n",
        "\n",
        "**Finance:**\n",
        "\n",
        "Neural networks are applied in predicting stock prices, credit scoring, fraud detection, and algorithmic trading.\n",
        "\n",
        "**Gaming and Entertainment:**\n",
        "\n",
        "Neural networks contribute to creating realistic characters, generating content, and improving the gaming experience through adaptive gameplay.\n",
        "\n",
        "**Recommendation Systems:**\n",
        "\n",
        "Neural networks power recommendation engines on platforms such as Netflix and Amazon, providing personalized content suggestions to users.\n",
        "\n",
        "**Robotics:**\n",
        "\n",
        "Neural networks are used in robotic control systems, enabling robots to learn and adapt to changing environments.\n",
        "\n",
        "**Cybersecurity:**\n",
        "\n",
        "Neural networks can be employed for intrusion detection, malware analysis, and anomaly detection in network traffic.\n",
        "\n",
        "**Environmental Monitoring:**\n",
        "\n",
        "Neural networks can analyze data from sensors and satellites to monitor environmental changes, predict natural disasters, and assess climate patterns.\n",
        "While neural networks offer significant advantages, it's important to note that they also come with challenges such as the need for large labeled datasets, computational resources, and potential overfitting. Nonetheless, ongoing research and advancements continue to address these challenges and expand the applications of neural networks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dgzR9Fjzqzyk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fu99wZesqvQz"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}